---
title: "capstone_google_project_cyclist_cleaning"
author: "Alisson Vinicius Salvador de Lima"
date: "2024-01-08"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

#  Cleaning and Ensuring Data Integrity


## Instaling and loading libraries

```{r install}
install.packages("skimr")
install.packages("tidyverse")
install.packages("janitor")
install.packages("stringr")
install.packages("lubridate")
```

## Loading Packages

```{r load packages}

library(tidyverse)
library(skimr)
library(janitor)
library(dplyr)
library(tidyr)        
library(ggplot2)
library(stringr)
library(lubridate)

```


## Loading data frames

```{r loading data}

df_2022_12 <- read_csv("data/202212_divvy_tripdata.csv")
df_2023_01 <- read_csv("data/202301_divvy_tripdata.csv")
df_2023_02 <- read_csv("data/202302_divvy_tripdata.csv")
df_2023_03 <- read_csv("data/202303_divvy_tripdata.csv")
df_2023_04 <- read_csv("data/202304_divvy_tripdata.csv")
df_2023_05 <- read_csv("data/202305_divvy_tripdata.csv")
df_2023_06 <- read_csv("data/202306_divvy_tripdata.csv")
df_2023_07 <- read_csv("data/202307_divvy_tripdata.csv")
df_2023_08 <- read_csv("data/202308_divvy_tripdata.csv")
df_2023_09 <- read_csv("data/202309_divvy_tripdata.csv")
df_2023_10 <- read_csv("data/202310_divvy_tripdata.csv")
df_2023_11 <- read_csv("data/202311_divvy_tripdata.csv")
```

## Consolidating data

The Microsoft Excel has been used to sort and filter each data set in order to find misspellings, missing values and other inconsistencies. Once it was identified (check the process on documentation), it was managed to consolidate all twelve data set into one unique data set.

```{r consolidate}

df_last_12_months <- rbind(df_2022_12, df_2023_01, df_2023_02, df_2023_03,
                           df_2023_04, df_2023_05, df_2023_06, df_2023_07,
                           df_2023_08, df_2023_09, df_2023_10, df_2023_11)
```

## Looking to the data type structure

Check column names

```{r names}

names(df_last_12_months)

```

Check data structure

```{r str}

str(df_last_12_months)

```

Get a glimpse of data

```{r glimpse}

glimpse(df_last_12_months)

```


## Visualizing Missing Values

```{r viz}

# Summarize data counting NA per columns
na_counts <- df_last_12_months %>% summarise_all(~ sum(is.na(.)))

# Bar plot for the columns missing values 
ggplot(data = gather(na_counts, key = "Column", value = "NA_count"), aes(x = Column, y = NA_count)) +
  geom_bar(stat = "identity", fill = "skyblue", color = "black") +
  labs(title = "NA Counts for Each Column", x = "Columns", y = "NA Count") +
  theme(axis.text.x = element_text(angle = 45))
```

Since there is just a few missing values on latitude and longitude they will be dropped. There is some strange values for end_lat and end_lng with zero, which is out of the range and it is going to be dropped too.

## Filtering rows without NA on position columns

```{r drop}
drop <- c('start_lat', 'start_lng', 'end_lat', 'end_lng')

# Filter rows with missing or zero values in any of the specified columns
filtered_df <- df_last_12_months[!(rowSums(is.na(df_last_12_months[drop]) | df_last_12_months[drop] == 0) > 0), ]
```

Check the range - It's correct now

```{r range}
range(filtered_df$start_lat)
range(filtered_df$start_lng)
range(filtered_df$end_lat)
range(filtered_df$end_lng)
```

Check for missings

```{r missing}
missing_1 <- filtered_df %>% filter(is.na(start_lat))
missing_2 <- filtered_df %>% filter(is.na(start_lng))
missing_3 <- filtered_df %>% filter(is.na(end_lat))
missing_4 <- filtered_df %>% filter(is.na(end_lng))

```
Check the environment chart to see that there is none messing values on these columns

## Ensuring data consistency

```{r consist}

data_cleaned <- filtered_df %>% clean_names(.)

```


## Missings on Station ID and Station names

There was a huge number of missing values on start_station_id, start_station_name, end_station_id and end_station_name. Since there is no way of ensuring the consistency of these values, this is the weakest point of these data sets, but it is possible to advance with the analyses using the coordinates that are very consistent

## Export the cleaned data set

```{r save}

write.csv(data_cleaned, "./data/cyclist_cleaned_consolidated_data_2024_01_08.csv", row.names = FALSE)
```

# Transform to Analyse

## Create New Variables

### Load Data

```{r load_2}

df_transf_01 <- read_csv("data/cyclist_cleaned_consolidated_data_2024_01_08.csv")

```


### Lentgh Time of Rides

```{r lentgh}

df_transf_01$ride_length <-  abs(df_transf_01$ended_at - df_transf_01$started_at)

```

Check

```{r lentgh check}

head(df_transf_01)

```

### Day of Week

```{r week}

df_transf_01$day_of_week <- weekdays(df_transf_01$started_at)

```

Check


```{r week check}

head(df_transf_01)

```

### Month of Ride

```{r month}

df_transf_01$ride_month <- month(df_transf_01$started_at)

```

Check

```{r month check}

head(df_transf_01)

```

## Day

```{r day}

df_transf_01$ride_day <- day(df_transf_01$started_at)

```

Check

```{r month check}

head(df_transf_01)

```

### Haversine distance between start and end of a ride using coordenates

#### Define Function

```{r distance}

# Function to calculate Haversine distance
haversine_distance <- function(lat1, lon1, lat2, lon2) {
  # Earth radius in kilometers
  R <- 6371

  # Convert degrees to radians
  dlat <- (lat2 - lat1) * pi / 180
  dlon <- (lon2 - lon1) * pi / 180

  # Haversine formula
  a <- sin(dlat/2)^2 + cos(lat1 * pi / 180) * cos(lat2 * pi / 180) * sin(dlon/2)^2
  c <- 2 * atan2(sqrt(a), sqrt(1-a))

  # Distance in kilometers
  distance <- R * c

  return(distance)
}

```

#### Calculate the distances in Kilometers

```{r distance calc}

df_transf_01$distance <- haversine_distance(df_transf_01$start_lat, df_transf_01$start_lng, df_transf_01$end_lat, df_transf_01$end_lng)

```

Check

```{r distance check}

head(df_transf_01)

```


## Save processed data

```{r process}

write.csv(df_transf_01, "data/processed_data_2024_01_08.csv", row.names = FALSE)

```

## New data for day information

```{r}

df_transf_02 <- df_transf_01

```

```{r}

df_transf_02$ride_hour <- hour(df_transf_01$started_at)

```

```{r}

write.csv(df_transf_02, "data/df_for_hour_analysis.csv", row.names = FALSE)

```


